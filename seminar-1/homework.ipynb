{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачать нейросети ResNet и написать короткую процедуру для предсказания класса изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот код загружает предварительно обученную модель ResNet50, использует её для предсказания классов изображения и выводит топ-3 наиболее вероятных класса. Убедитесь, что путь к изображению указан правильно в вызове функции predict_class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установите необходимые библиотеки:\n",
    "\n",
    "# pip install tensorflow keras pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m102967424/102967424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Загрузите предварительно обученную модель ResNet50 из Keras:\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Напишите функцию для предсказания класса изображения:\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_class(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    preds = model.predict(x)\n",
    "    print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Predicted: [('n04037443', 'racer', 0.9335238), ('n04285008', 'sports_car', 0.050977875), ('n02974003', 'car_wheel', 0.0060688853)]\n"
     ]
    }
   ],
   "source": [
    "# Используйте функцию predict_class для предсказания класса изображения:\n",
    "\n",
    "predict_class('nfs-need-for-speed-police-airplanes.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачать нейросеть BERT для лингвистических задач и реализовать процедуру классификации текстов (без оглядки на качество классификации)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот код предоставляет базовую структуру для работы с BERT и классификации текстов. Вы можете адаптировать его под свои нужды, изменяя параметры модели, методы предварительной обработки и классификации.\n",
    "\n",
    "Он позволяет классифицировать тексты на основе их семантического содержания, представленного в виде эмбеддингов, полученных от модели BERT/RuBERT. Такой подход может быть использован во многих задачах обработки естественного языка, таких как анализ тональности, классификация текстов по категориям и многое другое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установите библиотеку transformers, если у вас её ещё нет:\n",
    "\n",
    "# pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируйте необходимые модули и загрузите модель BERT. \n",
    "# В этом примере используется англоязычная версия BERT (bert-base-uncased), \n",
    "# но вы можете выбрать любую другую доступную модель.\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Чтобы использовать RuBERT вместо стандартной модели BERT в вашем коде, \n",
    "# вам нужно заменить 'bert-base-uncased' на идентификатор модели RuBERT. \n",
    "# Для этого воспользуйтесь моделью DeepPavlov/rubert-base-cased.\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "model = AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "# Здесь мы используем AutoTokenizer и AutoModel из библиотеки transformers, \n",
    "# чтобы автоматически определить тип токенизатора и модели, соответствующий \n",
    "# указанному идентификатору. Это позволяет легко переключаться между различными моделями, включая RuBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предварительная обработка текста\n",
    "# Преобразуйте ваши тексты в тензоры, используя токенизатор BERT.\n",
    "\n",
    "texts = [\"Наследственные заболевания сетчатки (НЗС), являясь орфанными заболеваниями, имеют высокую социальную значимость ввиду выраженных нарушений зрения и инвалидности пациентов в детском и трудоспособном возрасте. Известно более 100 нозологических форм НЗС, которые вызываются различными дефектами, обнаруженными примерно в 270 генах.\",\n",
    "         \"Автомобилисты, приходящие из поисковых систем на сайт магазина автозапчастей по запросам «как избежать перегрева двигателя» и «как защитить радиатор автомобиля от повреждений», найдут ответ в тексте по установке фальшрадиаторной сетки. Вероятно, и закажут запчасть именно на этом сайте.\"]\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение эмбеддингов из модели BERT\n",
    "# Выполните прямой проход через модель BERT, чтобы получить эмбеддинги для ваших текстов.\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Класс: [1]\n"
     ]
    }
   ],
   "source": [
    "# Классификация текстов\n",
    "# Для простейшей классификации можно использовать среднее значение эмбеддингов по последней размерности \n",
    "# и затем применить любой классификатор. Например, используйте логистическую регрессию из sklearn.\n",
    "\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Среднее значение эмбеддингов\n",
    "embeddings = last_hidden_states.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Пример меток классов\n",
    "labels = [0, 1]  # Замените на реальные метки\n",
    "\n",
    "# Обучение классификатора\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(embeddings, labels)\n",
    "\n",
    "# Предсказание классов для новых текстов\n",
    "new_texts = [\n",
    "    \"ракета\"\n",
    "    ]\n",
    "new_inputs = tokenizer(new_texts, return_tensors=\"pt\",\n",
    "                       padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    new_outputs = model(**new_inputs)\n",
    "    new_embeddings = new_outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "predictions = classifier.predict(new_embeddings)\n",
    "\n",
    "print(\"Класс:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
